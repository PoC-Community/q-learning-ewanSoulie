{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning\n",
    "### Introduction\n",
    "Today, you'll learn about QLearning using the gym api.\n",
    "> Gym is a standard API for reinforcement learning, and a diverse collection of reference environments.\n",
    "\n",
    "Reinforcement learning is a subset of Artificial Intelligence. Our AI will learn by playing the same game over and over again until it learns what it must do in order to win.\n",
    "\n",
    "In this workshop, you will:\n",
    "\n",
    "1. Learn about Q Tables\n",
    "2. Setup a gym environment\n",
    "3. Train an AI to solve the [FrozenLake environment](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "\n",
    "Let's start by importing the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Q-Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example of Q-Learning: \n",
    "\n",
    "![Example of a Q-Learning Environment](./images/example.png)\n",
    "\n",
    "A game where the AI must get to the ‚ÄúEnd‚Äù coordinates on the map in the shortest amount of time while avoiding the bombs and collecting the bonuses.\n",
    "\n",
    "A Q-Table for this environment could be visualized like this. \n",
    "\n",
    "![Example of a Q-Table](./images/qtable.png)\n",
    "\n",
    "With all of the possible actions listed horizontally and all of the possible states listed vertically.\n",
    "\n",
    "Write a function which generates an empty array of shape `x` * `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Initializing an empty table\n",
    "\n",
    "def init_q_table(x: int, y: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function must return a 2D matrix containing only zeros for values.\n",
    "    \"\"\"\n",
    "    return np.zeros((x, y))\n",
    "\n",
    "qTable = init_q_table(5, 4)\n",
    "\n",
    "print(\"Q-Table:\\n\" + str(qTable))\n",
    "\n",
    "assert(np.mean(qTable) == 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reinforcement learning, the AI receives a **reward** each time it **acts**:\n",
    "\n",
    "This reward is a number that will be interpreted by our AI:\n",
    "\n",
    "- If it is negative, the AI will learn that the action it made which led to this reward was probably not the best\n",
    "- If it is null or positive, the AI will learn that the action it made which led to this reward was probably smart and it should do it again.\n",
    "\n",
    "Here, the AI would likely receive a **negative reward** (-1) if it chooses to move to the right (because there's a bomb)\n",
    "\n",
    "However, if it went anywhere else, it would probably receive a **neutral reward** (0), since it would end up on blank squares.\n",
    "\n",
    "Assuming it decided to go to the right, the AI would encounter a bomb and the value for [Start, Move_Right] would decrease.\n",
    "\n",
    "\n",
    "Once we receive a reward, we can use the Q Formula below to update our Q Table's values. Our AI will use these values to learn what the most profitable action is at each state of the environment.\n",
    "\n",
    "![QFormula](./images/qformula.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translation:\n",
    "\n",
    "**Q<sup>new</sup>(s<sub>t</sub>, a<sub>t</sub>)** = the new value for the current State and Action on our Qtable\n",
    "\n",
    "**Q(s<sub>t</sub>, a<sub>t</sub>)** = the old value for the current State and Action on our QTable\n",
    "\n",
    "**Œ± (learning rate)** = a constant value we use for our algorithm's learning speed (usually a number between 0.5 and 0.05)\n",
    "\n",
    "**r<sub>t</sub> (reward)** = the reward received (as per the example above, -1 if you hit a bomb)\n",
    "\n",
    "**ùõæ (discount factor)** = another constant value we use to determine how good of a memory our AI has\n",
    "\n",
    "**max Q<sup>new</sup>(s<sub>t+1</sub>, a)** = the value of the most profitable action at the new state\n",
    "\n",
    "##### Now that you know how a Q Table updates itself using the Q Formula, let's try to implement this formula as a python function:\n",
    "(You can use the `LEARNING_RATE` and `DISCOUNT_RATE` constants in your formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.05\n",
    "DISCOUNT_RATE = 0.99\n",
    "\n",
    "def q_function(q_table: np.ndarray, state: int, action: int, reward: int, newState: int) -> float:\n",
    "  \"\"\"\n",
    "  This function implements the q_function equation.\n",
    "\n",
    "  It returns the updated Q-value for the current state-action pair.\n",
    "  \"\"\"\n",
    "  number = q_table[state, action] + LEARNING_RATE * (reward+ DISCOUNT_RATE * max(q_table[newState, :] - q_table[state, action]))\n",
    "  return number\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can test the Q Function to see if it works !\\\n",
    "If qTable[0, 1]'s value below is `-0.05`, congrats: you have successfully implemented the formula !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table after action:\n",
      "[[ 0.   -0.05  0.    0.  ]\n",
      " [ 0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "q_table = init_q_table(5,4)\n",
    "\n",
    "q_table[0, 1] = q_function(q_table, state=0, action=1, reward=-1, newState=3)\n",
    "\n",
    "print(\"Q-Table after action:\\n\" + str(q_table))\n",
    "\n",
    "assert(q_table[0, 1] == -LEARNING_RATE), f\"The Q function is incorrect: the value of qTable[0, 1] should be -{LEARNING_RATE}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up the GYM Environment\n",
    "\n",
    "#### What is Gym ?\n",
    "> Gym is a standard API for reinforcement learning, and a diverse collection of reference environments\n",
    ">\n",
    "> -- <cite>Gym Docs</cite>\n",
    "\n",
    "Let's get to the fun part:\n",
    "Read the documentation for FrozenLake [here](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "and find out how to load the environment.\n",
    "\n",
    "For now, we will set the `is_slippery` variable to `False` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write some code to load and make the FrozenLake environment:\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our environment is loaded, we need a function that performs a random action.\n",
    "\n",
    "An action is a number between 0 and the total number of possible actions.\n",
    "\n",
    "Try to find the value (inside `env`) that gives this total number of actions in the environment and store it in `total_actions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4}\n"
     ]
    }
   ],
   "source": [
    "total_actions = env.action_space.n\n",
    "\n",
    "assert(total_actions == 4), f\"There are a total of four possible actions in this environment. Your answer is {total_actions}\"\n",
    "print({total_actions})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use `total_actions` to make a function which returns a random action each time it is called !\n",
    "\n",
    "There is also another way to do it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_action(env):\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `random_action` function, you can run the code below which will display the first frame of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Image data of dtype object cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m observation, reward, done, _, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Displaying the first frame of the game\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m plt\u001b[39m.\u001b[39mimshow(env\u001b[39m.\u001b[39mrender())\n\u001b[1;32m     10\u001b[0m \u001b[39m# Printing game info\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactions: \u001b[39m\u001b[39m{\u001b[39;00menv\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mstates: \u001b[39m\u001b[39m{\u001b[39;00menv\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mn\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/matplotlib/pyplot.py:2695\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2689\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mimshow)\n\u001b[1;32m   2690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimshow\u001b[39m(\n\u001b[1;32m   2691\u001b[0m         X, cmap\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, norm\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m, aspect\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, interpolation\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2692\u001b[0m         alpha\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, origin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, extent\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2693\u001b[0m         interpolation_stage\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, filternorm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, filterrad\u001b[39m=\u001b[39m\u001b[39m4.0\u001b[39m,\n\u001b[1;32m   2694\u001b[0m         resample\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, url\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2695\u001b[0m     __ret \u001b[39m=\u001b[39m gca()\u001b[39m.\u001b[39mimshow(\n\u001b[1;32m   2696\u001b[0m         X, cmap\u001b[39m=\u001b[39mcmap, norm\u001b[39m=\u001b[39mnorm, aspect\u001b[39m=\u001b[39maspect,\n\u001b[1;32m   2697\u001b[0m         interpolation\u001b[39m=\u001b[39minterpolation, alpha\u001b[39m=\u001b[39malpha, vmin\u001b[39m=\u001b[39mvmin,\n\u001b[1;32m   2698\u001b[0m         vmax\u001b[39m=\u001b[39mvmax, origin\u001b[39m=\u001b[39morigin, extent\u001b[39m=\u001b[39mextent,\n\u001b[1;32m   2699\u001b[0m         interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[1;32m   2700\u001b[0m         filternorm\u001b[39m=\u001b[39mfilternorm, filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[1;32m   2701\u001b[0m         url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m: data} \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}),\n\u001b[1;32m   2702\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   2703\u001b[0m     sci(__ret)\n\u001b[1;32m   2704\u001b[0m     \u001b[39mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/matplotlib/__init__.py:1446\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m   1444\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1445\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1446\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39m\u001b[39mmap\u001b[39m(sanitize_sequence, args), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1448\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1449\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[1;32m   1450\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/matplotlib/axes/_axes.py:5663\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5655\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m   5656\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap\u001b[39m=\u001b[39mcmap, norm\u001b[39m=\u001b[39mnorm,\n\u001b[1;32m   5657\u001b[0m                       interpolation\u001b[39m=\u001b[39minterpolation, origin\u001b[39m=\u001b[39morigin,\n\u001b[1;32m   5658\u001b[0m                       extent\u001b[39m=\u001b[39mextent, filternorm\u001b[39m=\u001b[39mfilternorm,\n\u001b[1;32m   5659\u001b[0m                       filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[1;32m   5660\u001b[0m                       interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[1;32m   5661\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 5663\u001b[0m im\u001b[39m.\u001b[39mset_data(X)\n\u001b[1;32m   5664\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5665\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5666\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/matplotlib/image.py:701\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39msafe_masked_invalid(A, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    699\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39muint8 \u001b[39mand\u001b[39;00m\n\u001b[1;32m    700\u001b[0m         \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mcan_cast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, \u001b[39mfloat\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msame_kind\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[0;32m--> 701\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImage data of dtype \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m cannot be converted to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    702\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype))\n\u001b[1;32m    704\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    705\u001b[0m     \u001b[39m# If just one dimension assume scalar and apply colormap\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A[:, :, \u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "observation, info = env.reset()\n",
    "\n",
    "# Performing an action\n",
    "action = random_action(env)\n",
    "observation, reward, done, _, info = env.step(action)\n",
    "\n",
    "# Displaying the first frame of the game\n",
    "plt.imshow(env.render())\n",
    "\n",
    "# Printing game info\n",
    "print(f\"actions: {env.action_space.n}\\nstates: {env.observation_space.n}\")\n",
    "print(f\"Current state: {observation}\")\n",
    "\n",
    "# Closing the environment\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this environment, there are **4 possible actions** for each of the **16 possible states**.\\\n",
    "Feel free to play around with the code above to get a better understanding of the API.\n",
    "\n",
    "## 3. Solving the environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got a Q Table, Q Function and our environment, we can create our `game_loop`.\n",
    "\n",
    "Call `qFunc()` using the correct arguments it requires in the code below to update the `qTable[,]` using the values available in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_loop(env: gym.Env, q_table: np.ndarray, state: int, action: int) -> tuple:\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to see how the environment runs using your `game_loop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"human\")\n",
    "q_table = init_q_table(env.observation_space.n, env.action_space.n)\n",
    "\n",
    "state, info = env.reset()\n",
    "while (True):\n",
    "    env.render()\n",
    "    action = random_action(env)\n",
    "    q_table, state, done, reward = game_loop(env, q_table, state, action)\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to see if our AI learns, we will launch the environment 1000 times and see how the Q-Table evolves.\n",
    "\n",
    "Notice how in the code below, we do not call `env.render()` each frame because if we did, our 1000 iterations would take a **lot** of time to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 20000\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "q_table = init_q_table(env.observation_space.n, env.action_space.n)\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    state, info = env.reset()\n",
    "    while (True):\n",
    "        # This time, we won't render the game each frame because it would take too long\n",
    "        action = random_action(env)\n",
    "        q_table, state, done, reward = game_loop(env, q_table, state, action)\n",
    "        if done:\n",
    "            break\n",
    "env.close()\n",
    "\n",
    "# Printing the QTable result:\n",
    "for states in q_table:\n",
    "    for actions in states:\n",
    "        if (actions == max(states)):\n",
    "            print(\"\\033[4m\", end=\"\")\n",
    "        else:\n",
    "            print(\"\\033[0m\", end=\"\")\n",
    "        if (actions > 0):\n",
    "            print(\"\\033[92m\", end=\"\")\n",
    "        else:\n",
    "            print(\"\\033[00m\", end=\"\")\n",
    "        print(round(actions, 3), end=\"\\t\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great ! We now have a nice Q-Table that indicates which action is best for each state."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it would probably be better for our agent to choose actions based on its experience, rather than at random.\n",
    "Write a function which chooses the best action for each given state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_action(q_table: np.ndarray, state: int) -> int:\n",
    "    \"\"\"\n",
    "    Write a function which finds the best action for the given state.\n",
    "\n",
    "    It should return its index.\n",
    "    \"\"\"\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to see the result of our training !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"human\")\n",
    "\n",
    "state, info = env.reset()\n",
    "while (True):\n",
    "    env.render()\n",
    "    action = best_action(q_table, state)\n",
    "    q_table, state, done, reward = game_loop(env, q_table, state, action)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If all went well, our AI should easily reach its goal !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the AI should probably try to explore all the different possibilities before it starts optimising its gain by following the Q Table...\\\n",
    "In order to solve this problem, we can use the Epsilon-Greedy strategy!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Epsilon Greedy ?\n",
    "\n",
    "Epsilon greedy is a strategy used to make sure that our AI explores its environment and doesn‚Äôt miss out on some cool easter eggs !\n",
    "\n",
    "![EpsilonGreedy](https://steadfast-ragdoll-d83.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fa8444925-9940-46b2-b9e3-aae5ca792a74%2FUntitled.png?table=block&id=ce90b632-5d78-4cb0-8101-b326b91bf25d&spaceId=a008bb22-92df-498d-b32c-15988ddb70b9&width=770&userId=&cache=v2)\n",
    "\n",
    "<cite>[source](https://medium.com/analytics-vidhya/the-epsilon-greedy-algorithm-for-reinforcement-learning-5fe6f96dc870)</cite>\n",
    "\n",
    "```\n",
    "if random(0, 1) > epsilon-greedy:\n",
    "\treturn bestKnownAction\n",
    "else:\n",
    "\treturn randomAction\n",
    "```\n",
    "\t\t\n",
    "The higher the epsilon-greedy value is (from 0 to 1), the higher chance of the AI picking an action at random.\n",
    "\n",
    "The lower the epsilon-greedy value is (from 0 to 1 still), the higher chance of the AI picking an action it considers the most rewarding.\n",
    "\n",
    "In other words, with a high epsilon-greedy value, the robot pictured above might go to the right whereas it would probably choose the left path if it had a lower epsilon-greedy value, therefore missing out on a special discovery !\n",
    "\n",
    "\n",
    "\n",
    "Try to implement it into the following function that we'll use to determine which action the AI will choose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(epsilon: float, q_table: np.ndarray, state: int, env: gym.Env) -> int:\n",
    "    \"\"\"\n",
    "    Write a function to either return a random action or the best action using the functions\n",
    "    defined earlier.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function, the AI will explore all possible actions before it chooses the best state, reducing the risk of it missing some golden solutions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we're not done...\\\n",
    "You might have noticed that when we load our environment, we give it a certain argument:\\\n",
    "`is_slippery=False`\n",
    "\n",
    "This argument makes the game far easier !\n",
    "If you want a real challenge, set it to true."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the environment will be much harder to solve if the ice is slippery. Our agent's movements will be unpredictable and it will be impossible to make a 100% accurate AI.\n",
    "\n",
    "Therefore, keep track of the number of times the AI gets to its goal during our testing phase.\n",
    "\n",
    "##### Try to get the highest accuracy possible !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode=\"rgb_array\")\n",
    "\n",
    "q_table = init_q_table(env.observation_space.n, env.action_space.n)\n",
    "\n",
    "# Training the AI\n",
    "epsilon = 1.0\n",
    "for i in range(10000):\n",
    "    epsilon = max(epsilon - 0.0001, 0)\n",
    "    state, info = env.reset()\n",
    "    while (True):\n",
    "        action = choose_action(epsilon, q_table, state, env)\n",
    "        q_table, state, done, reward = game_loop(env, q_table, state, action)\n",
    "        if done:\n",
    "            break\n",
    "# Testing the AI\n",
    "wins = 0.0\n",
    "for i in range(100):\n",
    "    state, info = env.reset()\n",
    "    while (True):\n",
    "        action = choose_action(0, q_table, state, env)\n",
    "        _, state, done, reward = game_loop(env, q_table, state, action)\n",
    "        if done:\n",
    "            if (reward > 0):\n",
    "                wins += 1\n",
    "            break\n",
    "\n",
    "print(f\"{round(wins / (i+1) * 100, 2)}% winrate\")\n",
    "print(wins)\n",
    "\n",
    "# Displaying the last frame of the game\n",
    "plt.imshow(env.render())\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job ! You completed this workshop !\n",
    "If you want to continue in the wonderful world of reinforcement learning, you could try:\n",
    "- Try and increase the accuracy by using different methods of reinforcement learning.\n",
    "- A custom map for Frozen Lake. For example: `gym.make('FrozenLake-v1', map_name=\"8x8\", is_slippery=True)`\n",
    "- A different environment from https://gymnasium.farama.org/\n",
    "- Going **deeper** with https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "- Trying what you've learned with games you know and love:\n",
    "    - https://gymnasium.farama.org/environments/atari/\n",
    "    - https://pypi.org/project/gym-super-mario-bros/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cba5719d159acad27bb7f37d182d27b1d38df28115fbd0d331121e7aca3605"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
